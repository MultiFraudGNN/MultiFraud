{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cc6ed1",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca059856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:53:02.463327Z",
     "start_time": "2022-06-08T00:52:51.891831Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbc7da",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24517e",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from imp import reload\n",
    "from utils import synthetic_structsim, featgen\n",
    "\n",
    "reload(synthetic_structsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(graph_list, p):\n",
    "    \"\"\" Perturb the list of (sparse) graphs by adding/removing edges.\n",
    "    Args:\n",
    "        p: proportion of added edges based on current number of edges.\n",
    "    Returns:\n",
    "        A list of graphs that are perturbed from the original graphs.\n",
    "    \"\"\"\n",
    "    perturbed_graph_list = []\n",
    "    for G_original in graph_list:\n",
    "        G = G_original.copy()\n",
    "        edge_count = int(G.number_of_edges() * p)\n",
    "        # randomly add the edges between a pair of nodes without an edge.\n",
    "        for _ in range(edge_count):\n",
    "            while True:\n",
    "                u = np.random.randint(0, G.number_of_nodes())\n",
    "                v = np.random.randint(0, G.number_of_nodes())\n",
    "                if (not G.has_edge(u, v)) and (u != v):\n",
    "                    break\n",
    "            G.add_edge(u, v)\n",
    "        perturbed_graph_list.append(G)\n",
    "    return perturbed_graph_list\n",
    "\n",
    "def gen_syn1(nb_shapes=70, width_basis=2600, feature_generator=None, m=5):\n",
    "    \"\"\" Synthetic Graph #1:\n",
    "    Start with Barabasi-Albert graph and attach house-shaped subgraphs.\n",
    "    Args:\n",
    "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
    "        width_basis       :  The width of the basis graph (here 'Barabasi-Albert' random graph).\n",
    "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
    "        m                 :  number of edges to attach to existing node (for BA graph)\n",
    "    Returns:\n",
    "        G                 :  A networkx graph\n",
    "        role_id           :  A list with length equal to number of nodes in the entire graph (basis\n",
    "                          :  + shapes). role_id[i] is the ID of the role of node i. It is the label.\n",
    "        name              :  A graph identifier\n",
    "    \"\"\"\n",
    "    basis_type = \"ba\"\n",
    "    list_shapes = [[\"house\"]] * nb_shapes\n",
    "\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "    G, role_id, _ = synthetic_structsim.build_graph(\n",
    "        width_basis, basis_type, list_shapes, start=0, m=5\n",
    "    )\n",
    "    G = perturb([G], 0.01)[0]\n",
    "\n",
    "    if feature_generator is None:\n",
    "        feature_generator = featgen.ConstFeatureGen(1)\n",
    "    feature_generator.gen_node_features(G)\n",
    "\n",
    "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
    "    return G, role_id, name\n",
    "\n",
    "\n",
    "def gen_syn4(nb_shapes=20, width_basis=9, feature_generator=None, m=4):\n",
    "    \"\"\" Synthetic Graph #4:\n",
    "\n",
    "    Start with a tree and attach cycle-shaped subgraphs.\n",
    "\n",
    "    Args:\n",
    "        nb_shapes         :  The number of shapes (here 'houses') that should be added to the base graph.\n",
    "        width_basis       :  The width of the basis graph (here a random 'Tree').\n",
    "        feature_generator :  A `FeatureGenerator` for node features. If `None`, add constant features to nodes.\n",
    "        m                 :  The tree depth.\n",
    "\n",
    "    Returns:\n",
    "        G                 :  A networkx graph\n",
    "        role_id           :  Role ID for each node in synthetic graph\n",
    "        name              :  A graph identifier\n",
    "    \"\"\"\n",
    "    basis_type = \"tree\"\n",
    "    list_shapes = [[\"cycle\", 6]] * nb_shapes\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "    G, role_id, plugins = synthetic_structsim.build_graph(\n",
    "        width_basis, basis_type, list_shapes, start=0\n",
    "    )\n",
    "    G = perturb([G], 0.01)[0]\n",
    "\n",
    "    if feature_generator is None:\n",
    "        feature_generator = featgen.ConstFeatureGen(1)\n",
    "    feature_generator.gen_node_features(G)\n",
    "\n",
    "    name = basis_type + \"_\" + str(width_basis) + \"_\" + str(nb_shapes)\n",
    "\n",
    "#     path = os.path.join(\"log/syn4_base_h20_o20\")\n",
    "#     writer = SummaryWriter(path)\n",
    "#     io_utils.log_graph(writer, G, \"graph/full\")\n",
    "\n",
    "    return G, role_id, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebe9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, role_id, name = gen_syn4()\n",
    "print(len(role_id))\n",
    "trans_G, trans_role_id, trans_name = gen_syn4(nb_shapes=80, width_basis=11)\n",
    "\n",
    "role_id = np.array(role_id)\n",
    "fraud_comp_index = torch.Tensor(G.nodes()).int()[role_id > 0]\n",
    "print(len(fraud_comp_index))\n",
    "print(len(fraud_comp_index) / len(role_id))\n",
    "fraud_comp_index_attach = torch.Tensor(G.nodes()).int()[role_id == 11]\n",
    "normal_comp_index = torch.Tensor(G.nodes()).int()[role_id == 0]\n",
    "\n",
    "trans_role_id = np.array(trans_role_id)\n",
    "fraud_tran_index = torch.Tensor(trans_G.nodes()).int()[trans_role_id > 0]\n",
    "fraud_tran_index_attach = torch.Tensor(trans_G.nodes()).int()[trans_role_id == 11]\n",
    "normal_tran_index = torch.Tensor(trans_G.nodes()).int()[trans_role_id == 0]\n",
    "\n",
    "print(len(trans_role_id))\n",
    "print(len(fraud_tran_index))\n",
    "print(len(fraud_tran_index) / len(trans_role_id))\n",
    "\n",
    "transactions_source = []\n",
    "transactions_target = []\n",
    "companies_index = [[] for i in range(len(role_id))] \n",
    "companies_length = []\n",
    "for index, label in enumerate(trans_role_id):\n",
    "    if label > 0:\n",
    "        a = random.randint(0, len(fraud_comp_index)-1)\n",
    "        b = random.randint(0, len(fraud_comp_index)-1)\n",
    "        companies_index[fraud_comp_index[a].item()].append(index + 1)\n",
    "        companies_index[fraud_comp_index[b].item()].append(index + 1)\n",
    "        transactions_source.append(fraud_comp_index[a].item())\n",
    "        transactions_target.append(fraud_comp_index[b].item())\n",
    "    else:\n",
    "        a = random.randint(0, len(role_id)-1)\n",
    "        b = random.randint(0, len(role_id)-1)\n",
    "        companies_index[a].append(index + 1)\n",
    "        companies_index[b].append(index + 1)\n",
    "        transactions_source.append(a)\n",
    "        transactions_target.append(b)\n",
    "\n",
    "maxL = 0\n",
    "for comp in companies_index:\n",
    "    companies_length.append(len(comp))\n",
    "    maxL = max(maxL, len(comp))\n",
    "\n",
    "for comp in companies_index:\n",
    "    comp += [0]*(maxL - len(comp))\n",
    "\n",
    "companies_length = torch.LongTensor(companies_length)\n",
    "\n",
    "# companies_index = torch.Tensor(companies_index).long()\n",
    "transactions_index = torch.Tensor([transactions_source, transactions_target]).long()\n",
    "\n",
    "random_mu = [0.0] * 6\n",
    "random_sigma = [0.1] * 6\n",
    "\n",
    "mu_1, sigma_1 = np.array([0] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
    "mu_2, sigma_2 = np.array([0.5] * 2 + random_mu), np.array([0.5] * 2 + random_sigma)\n",
    "\n",
    "# feat = np.random.multivariate_normal(mu_1, np.diag(sigma_1), len(fraud_comp_index))\n",
    "# feat2 = np.random.multivariate_normal(mu_2, np.diag(sigma_2), len(role_id) - len(fraud_comp_index))\n",
    "\n",
    "feat = np.random.normal(mu_1, sigma_1, (len(fraud_comp_index),8))\n",
    "feat2 = np.random.normal(mu_2, sigma_2, (len(role_id) - len(fraud_comp_index),8))\n",
    "\n",
    "t_feat = np.random.normal(mu_1, sigma_1, (len(fraud_tran_index),8))\n",
    "t_feat2 = np.random.normal(mu_2, sigma_2, (len(trans_role_id) - len(fraud_tran_index),8))\n",
    "\n",
    "company_node_feature = []\n",
    "transaction_node_feature = []\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for index in range(len(role_id)):\n",
    "    if role_id[index] > 0:\n",
    "        company_node_feature.append(feat[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        company_node_feature.append(feat2[j])\n",
    "        j += 1\n",
    "        \n",
    "i = 0\n",
    "j = 0\n",
    "for index in range(len(trans_role_id)):\n",
    "    if trans_role_id[index] > 0:\n",
    "        transaction_node_feature.append(t_feat[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        transaction_node_feature.append(t_feat2[j])\n",
    "        j += 1\n",
    "\n",
    "\n",
    "company_node_feature = torch.Tensor(company_node_feature)\n",
    "transaction_node_feature = torch.Tensor(transaction_node_feature)\n",
    "\n",
    "new_edge_index = []\n",
    "new_egde_type = []\n",
    "new_sources = []\n",
    "new_targets = []\n",
    "for i in G.edges():\n",
    "    new_sources.append(i[0])\n",
    "    new_targets.append(i[1])\n",
    "    new_egde_type.append(0)\n",
    "new_edge_index = [new_sources + new_targets, new_targets + new_sources]\n",
    "\n",
    "new_edge_index_2 = []\n",
    "new_sources_2 = []\n",
    "new_targets_2 = []\n",
    "for i in trans_G.edges():\n",
    "    new_sources_2.append(i[0])\n",
    "    new_targets_2.append(i[1])\n",
    "new_edge_index_2 = [new_sources_2 + new_targets_2, new_targets_2 + new_sources_2]\n",
    "\n",
    "new_edge_index = torch.LongTensor(new_edge_index)\n",
    "new_edge_index_2 = torch.LongTensor(new_edge_index_2)\n",
    "new_egde_type = torch.LongTensor(new_egde_type + new_egde_type)\n",
    "\n",
    "node_mapping = []\n",
    "for i in range(len(company_node_feature)):\n",
    "    node_mapping.append([i])\n",
    "node_mapping = torch.LongTensor(node_mapping)\n",
    "\n",
    "transaction_node_mapping = []\n",
    "for i in range(len(transaction_node_feature)):\n",
    "    transaction_node_mapping.append([i])\n",
    "transaction_node_mapping = torch.LongTensor(transaction_node_mapping)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "role_id[role_id > 0] = 1\n",
    "trans_role_id[trans_role_id > 0] = 1\n",
    "\n",
    "x_train_new1, x_test_new1, y_train_new1, y_test_new1 = train_test_split(list(G.nodes()), role_id, test_size=0.2, random_state=0, shuffle=True, stratify=role_id)\n",
    "x_train_new2, x_test_new2, y_train_new2, y_test_new2 = train_test_split(list(trans_G.nodes()), trans_role_id, test_size=0.2, random_state=0, shuffle=True, stratify=trans_role_id)\n",
    "\n",
    "# sio.savemat(\"dataset/synthetic_company_feature.mat\", {'matrix': nx.to_numpy_array(G) , 'features':company_node_feature.numpy(), 'label': np.array(role_id), 'X_train': x_train_new1, 'X_test':x_test_new1,'y_train':y_train_new1,'y_test':y_test_new1})\n",
    "# sio.savemat(\"dataset/synthetic_transaction_feature.mat\", {'matrix': nx.to_numpy_array(trans_G), 'features':transaction_node_feature.numpy(), 'label': np.array(trans_role_id),'X_train': x_train_new2, 'X_test':x_test_new2,'y_train':y_train_new2,'y_test':y_test_new2})\n",
    "\n",
    "x_train_new1 = torch.Tensor(x_train_new1).long()\n",
    "x_train_new2 = torch.Tensor(x_train_new2).long()\n",
    "y_train_new1 = torch.Tensor(y_train_new1).long()\n",
    "y_train_new2 = torch.Tensor(y_train_new2).long()\n",
    "x_test_new1 = torch.Tensor(x_test_new1).long()\n",
    "x_test_new2 = torch.Tensor(x_test_new2).long()\n",
    "y_test_new1 = torch.Tensor(y_test_new1).long()\n",
    "y_test_new2 = torch.Tensor(y_test_new2).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0dba89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T15:37:28.295935Z",
     "start_time": "2022-06-09T15:37:28.293892Z"
    }
   },
   "source": [
    "## Real-World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_trans_data = pd.read_csv('dataset/bs140513_032310.csv')\n",
    "real_trans_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_trans_data = real_trans_data.sample(n=50000)\n",
    "real_trans_data.customer.nunique()\n",
    "\n",
    "real_trans_data = real_trans_data.reset_index(drop=True)\n",
    "\n",
    "real_trans_data[\"merchant_amount\"] = real_trans_data[\"merchant\"] + real_trans_data[\"amount\"].map(str)\n",
    "real_trans_data[\"merchant_amount\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = real_trans_data.groupby('merchant_amount')\n",
    "real_trans_edge_source = []\n",
    "real_trans_edge_target = []\n",
    "real_trans_edge_type = []\n",
    "rel_trans_merchant_matrix = np.zeros((real_trans_data.shape[0], real_trans_data.shape[0]), dtype = bool)\n",
    "for index, row in real_trans_data.iterrows():\n",
    "    for target in list(gp.groups[row[\"merchant_amount\"]]):\n",
    "        real_trans_edge_source.append(index)\n",
    "        real_trans_edge_target.append(target)\n",
    "        real_trans_edge_source.append(target)\n",
    "        real_trans_edge_target.append(index)\n",
    "        real_trans_edge_type.append(0)\n",
    "        real_trans_edge_type.append(0)\n",
    "        rel_trans_merchant_matrix[index][target] = True\n",
    "        rel_trans_merchant_matrix[target][index] = True\n",
    "\n",
    "real_trans_edge_index = [real_trans_edge_source, real_trans_edge_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_trans_labels = real_trans_data['fraud'].tolist()\n",
    "\n",
    "real_trans_data['age'] = real_trans_data['age'].map(lambda x: str(x)[1:-1])\n",
    "real_trans_data['age'] = pd.to_numeric(real_trans_data['age'], errors='coerce')\n",
    "mean_val = real_trans_data['age'].mean()\n",
    "real_trans_data['age'].fillna(mean_val, inplace=True)\n",
    "\n",
    "real_trans_data = real_trans_data.join(pd.get_dummies(real_trans_data.gender))\n",
    "real_trans_data = real_trans_data.join(pd.get_dummies(real_trans_data.category))\n",
    "\n",
    "strs = real_trans_data[\"customer\"].value_counts()\n",
    "value_map = dict((v, i) for i,v in enumerate(strs.index))\n",
    "real_trans_data.replace({\"customer\":value_map} ,inplace=True)\n",
    "strs = real_trans_data[\"merchant\"].value_counts()\n",
    "value_map = dict((v, i) for i,v in enumerate(strs.index))\n",
    "real_trans_data.replace({\"merchant\":value_map} ,inplace=True)\n",
    "\n",
    "real_trans_data.drop('step', axis=1, inplace=True)\n",
    "real_trans_data.drop('zipMerchant', axis=1, inplace=True)\n",
    "real_trans_data.drop('zipcodeOri', axis=1, inplace=True)\n",
    "real_trans_data.drop('category', axis=1, inplace=True)\n",
    "real_trans_data.drop('fraud', axis=1, inplace=True)\n",
    "real_trans_data.drop('gender', axis=1, inplace=True)\n",
    "real_trans_data.drop('merchant_amount', axis=1, inplace=True)\n",
    "real_trans_features = real_trans_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba607dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edge_index = np.load('dataset/real_company_edge_index.npy')\n",
    "new_egde_type = list(np.load('dataset/real_company_new_egde_type.npy'))\n",
    "company_node_feature = np.load('dataset/rea_company_features.npy')\n",
    "role_id = np.load('dataset/rea_company_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_comp_index = torch.Tensor(range(len(role_id))).int()[role_id > 0]\n",
    "print(len(fraud_comp_index))\n",
    "print(len(fraud_comp_index) / len(role_id))\n",
    "normal_comp_index = torch.Tensor(range(len(role_id))).int()[role_id == 0]\n",
    "\n",
    "trans_role_id = np.array(real_trans_labels)\n",
    "fraud_tran_index = torch.Tensor(range(len(trans_role_id))).int()[trans_role_id > 0]\n",
    "normal_tran_index = torch.Tensor(range(len(trans_role_id))).int()[trans_role_id == 0]\n",
    "\n",
    "print(len(fraud_tran_index))\n",
    "print(len(fraud_tran_index) / len(trans_role_id))\n",
    "\n",
    "fraud_customer = []\n",
    "normal_customer = []\n",
    "gp_2 = real_trans_data.groupby('customer')\n",
    "keys = [key for key, _ in gp_2]\n",
    "neigh_dict = {}\n",
    "for key in keys:\n",
    "    flag = False\n",
    "    for index in list(gp_2.groups[key]):\n",
    "        if real_trans_labels[index] == 1:\n",
    "            flag = True\n",
    "            break\n",
    "    if flag:\n",
    "        fraud_customer.append(key)\n",
    "    else:\n",
    "        normal_customer.append(key)\n",
    "\n",
    "fraud_customer_index = random.sample(list(fraud_comp_index.numpy()), len(fraud_customer))\n",
    "normal_customer_index = random.sample(list(normal_comp_index.numpy()), len(normal_customer))\n",
    "\n",
    "fraud_customer_mapper = {}\n",
    "normal_customer_mapper = {}\n",
    "\n",
    "for k, v in zip(fraud_customer, fraud_customer_index):\n",
    "    fraud_customer_mapper[k] = v\n",
    "for k, v in zip(normal_customer, normal_customer_index):\n",
    "    normal_customer_mapper[k] = v\n",
    "\n",
    "transactions_source = []\n",
    "transactions_target = []\n",
    "companies_index = [[] for i in range(len(role_id))] \n",
    "companies_length = []\n",
    "for index, label in enumerate(trans_role_id):\n",
    "    if real_trans_data.iloc[index ,0] in fraud_customer_mapper:\n",
    "        companies_index[fraud_customer_mapper[real_trans_data.iloc[index ,0]]].append(index + 1)\n",
    "        transactions_source.append(fraud_customer_mapper[real_trans_data.iloc[index ,0]])\n",
    "    else:\n",
    "        companies_index[normal_customer_mapper[real_trans_data.iloc[index ,0]]].append(index + 1)\n",
    "        transactions_source.append(normal_customer_mapper[real_trans_data.iloc[index ,0]])\n",
    "\n",
    "maxL = 0\n",
    "for comp in companies_index:\n",
    "    companies_length.append(len(comp))\n",
    "    maxL = max(maxL, len(comp))\n",
    "\n",
    "for comp in companies_index:\n",
    "    comp += [0]*(maxL - len(comp))\n",
    "\n",
    "companies_length = torch.LongTensor(companies_length)\n",
    "\n",
    "# companies_index = torch.Tensor(companies_index).long()\n",
    "transactions_index = torch.Tensor(transactions_source).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_node_feature = real_trans_features\n",
    "\n",
    "company_node_feature = torch.Tensor(company_node_feature)\n",
    "transaction_node_feature = torch.Tensor(transaction_node_feature)\n",
    "\n",
    "new_edge_index = torch.LongTensor(new_edge_index)\n",
    "new_edge_index_2 = torch.LongTensor(real_trans_edge_index)\n",
    "new_egde_type = torch.LongTensor(new_egde_type + new_egde_type)\n",
    "real_trans_edge_type = torch.LongTensor(real_trans_edge_type)\n",
    "\n",
    "node_mapping = []\n",
    "for i in range(len(company_node_feature)):\n",
    "    node_mapping.append([i])\n",
    "node_mapping = torch.LongTensor(node_mapping)\n",
    "\n",
    "transaction_node_mapping = []\n",
    "for i in range(len(transaction_node_feature)):\n",
    "    transaction_node_mapping.append([i])\n",
    "transaction_node_mapping = torch.LongTensor(transaction_node_mapping)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "role_id[role_id > 0] = 1\n",
    "trans_role_id[trans_role_id > 0] = 1\n",
    "\n",
    "real_company_X_y_train_test = np.load('dataset/real_company_X_y_train_test.npy',allow_pickle=True)\n",
    "x_train_new1, x_test_new1, y_train_new1, y_test_new1 = real_company_X_y_train_test[0],real_company_X_y_train_test[1],real_company_X_y_train_test[2],real_company_X_y_train_test[3]\n",
    "x_train_new2, x_test_new2, y_train_new2, y_test_new2 = train_test_split(range(len(trans_role_id)), trans_role_id, test_size=0.2, random_state=0, shuffle=True, stratify=trans_role_id)\n",
    "\n",
    "# import scipy.io as sio\n",
    "# sio.savemat(\"dataset/synthetic_company_feature.mat\", {'matrix': nx.to_numpy_array(G) , 'features':company_node_feature.numpy(), 'label': np.array(role_id), 'X_train': x_train_new1, 'X_test':x_test_new1,'y_train':y_train_new1,'y_test':y_test_new1})\n",
    "# sio.savemat(\"dataset/synthetic_transaction_feature.mat\", {'matrix': nx.to_numpy_array(trans_G), 'features':transaction_node_feature.numpy(), 'label': np.array(trans_role_id),'X_train': x_train_new2, 'X_test':x_test_new2,'y_train':y_train_new2,'y_test':y_test_new2})\n",
    "\n",
    "x_train_new1 = torch.Tensor(x_train_new1).long()\n",
    "x_train_new2 = torch.Tensor(x_train_new2).long()\n",
    "y_train_new1 = torch.Tensor(y_train_new1).long()\n",
    "y_train_new2 = torch.Tensor(y_train_new2).long()\n",
    "x_test_new1 = torch.Tensor(x_test_new1).long()\n",
    "x_test_new2 = torch.Tensor(x_test_new2).long()\n",
    "y_test_new1 = torch.Tensor(y_test_new1).long()\n",
    "y_test_new2 = torch.Tensor(y_test_new2).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdc26c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e4513",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7e611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:17.522687Z",
     "start_time": "2022-06-08T00:58:17.517805Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_feature = 16\n",
    "hidden_units = [16, 16]\n",
    "learning_rate = 0.0003\n",
    "dropout_rate = 0\n",
    "num_epochs = 15\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "checkpoint_interval = 10\n",
    "\n",
    "def create_ffn(num_feats, dropout_rate = 0, name=None):\n",
    "    fnn_layers = []\n",
    "    fnn_layers.append(torch.nn.BatchNorm1d(num_feats))\n",
    "    fnn_layers.append(torch.nn.Dropout(p=dropout_rate))\n",
    "    fnn_layers.append(torch.nn.Linear(num_feats, hidden_feature))\n",
    "    fnn_layers.append(torch.nn.GELU())\n",
    "    fnn_layers.append(torch.nn.BatchNorm1d(hidden_feature))\n",
    "    fnn_layers.append(torch.nn.Dropout(p=dropout_rate))\n",
    "    fnn_layers.append(torch.nn.Linear(hidden_feature, hidden_feature))\n",
    "    fnn_layers.append(torch.nn.GELU())   \n",
    "\n",
    "    return torch.nn.Sequential(*fnn_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887c397",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe88bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:18.559770Z",
     "start_time": "2022-06-08T00:58:18.554648Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphConvLayer(MessagePassing):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                ):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        \n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.shape[0])\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96b678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:20.430518Z",
     "start_time": "2022-06-08T00:58:20.424840Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    " \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    " \n",
    " \n",
    "def create_src_lengths_mask(\n",
    "    batch_size: int, src_lengths: Tensor, max_src_len: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate boolean mask to prevent attention beyond the end of source\n",
    "    Inputs:\n",
    "      batch_size : int\n",
    "      src_lengths : [batch_size] of sentence lengths\n",
    "      max_src_len: Optionally override max_src_len for the mask\n",
    "    Outputs:\n",
    "      [batch_size, max_src_len]\n",
    "    \"\"\"\n",
    "    if max_src_len is None:\n",
    "        max_src_len = int(src_lengths.max())\n",
    "    src_indices = torch.arange(0, max_src_len).unsqueeze(0).type_as(src_lengths)\n",
    "    src_indices = src_indices.expand(batch_size, max_src_len)\n",
    "    src_lengths = src_lengths.unsqueeze(dim=1).expand(batch_size, max_src_len)\n",
    "    # returns [batch_size, max_seq_len]\n",
    "    return (src_indices < src_lengths).int().detach()\n",
    " \n",
    " \n",
    "def masked_softmax(scores, src_lengths, src_length_masking=True):\n",
    "    \"\"\"Apply source length masking then softmax.\n",
    "    Input and output have shape bsz x src_len\"\"\"\n",
    "    if src_length_masking:\n",
    "        bsz, max_src_len = scores.size()\n",
    "        # compute masks\n",
    "        src_mask = create_src_lengths_mask(bsz, src_lengths)\n",
    "        # Fill pad positions with -inf\n",
    "        scores = scores.masked_fill(src_mask == 0, -np.inf)\n",
    " \n",
    "    # Cast to float and then back again to prevent loss explosion under fp16.\n",
    "    return F.softmax(scores.float(), dim=-1).type_as(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d285c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:21.937798Z",
     "start_time": "2022-06-08T00:58:21.931973Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    " \n",
    "# s(x, q) = v.T * tanh (W * x + b)\n",
    "class MLPAttentionNetwork(nn.Module):\n",
    " \n",
    "    def __init__(self, hidden_dim, attention_dim, src_length_masking=True):\n",
    "        super(MLPAttentionNetwork, self).__init__()\n",
    " \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.src_length_masking = src_length_masking\n",
    " \n",
    "        # W * x + b\n",
    "        self.proj_w = nn.Linear(self.hidden_dim, self.attention_dim, bias=True)\n",
    "        # v.T\n",
    "        self.proj_v = nn.Linear(self.attention_dim, 1, bias=False)\n",
    " \n",
    "    def forward(self, x, x_lengths):\n",
    "        \"\"\"\n",
    "        :param x: seq_len * batch_size * hidden_dim\n",
    "        :param x_lengths: batch_size\n",
    "        :return: batch_size * seq_len, batch_size * hidden_dim\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        # (seq_len * batch_size, hidden_dim)\n",
    "        # flat_inputs = x.view(-1, self.hidden_dim)\n",
    "        flat_inputs = x.reshape(-1, self.hidden_dim)\n",
    "        # (seq_len * batch_size, attention_dim)\n",
    "        mlp_x = self.proj_w(flat_inputs)\n",
    "        # (batch_size, seq_len)\n",
    "        att_scores = self.proj_v(mlp_x).view(seq_len, batch_size).t()\n",
    "        # (seq_len, batch_size)\n",
    "        normalized_masked_att_scores = masked_softmax(\n",
    "            att_scores, x_lengths, self.src_length_masking\n",
    "        ).t()\n",
    "        # (batch_size, hidden_dim)\n",
    "        attn_x = (x * normalized_masked_att_scores.unsqueeze(2)).sum(0)\n",
    " \n",
    "        return normalized_masked_att_scores.t(), attn_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692bdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:23.960107Z",
     "start_time": "2022-06-08T00:58:23.952411Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    " \n",
    "class BiLSTMAttentionNetwork(nn.Module):\n",
    " \n",
    "    def __init__(self, num_vocab, embedding_dim, hidden_dim, num_layers, bidirectional, attention_dim, num_classes):\n",
    " \n",
    "        super(BiLSTMAttentionNetwork, self).__init__()\n",
    " \n",
    "        # 词表长度，实际单词数量+填充占位符（1）\n",
    "        self.num_vocab = num_vocab\n",
    "        # 最大序列长度\n",
    "        # self.max_len = max_len\n",
    "        # 单词隐向量维度\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # LSTM中隐层的维度\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 循环神经网络的层数\n",
    "        self.num_layers = num_layers\n",
    "        # 是否使用双向RNN，布尔值\n",
    "        self.bidirectional = bidirectional\n",
    "        # 注意力层参数维度\n",
    "        self.attention_dim = attention_dim\n",
    "        # 标签数量\n",
    "        self.num_classes = num_classes\n",
    "        # Embedding层\n",
    "        self.embedding_layer = nn.Embedding(self.num_vocab, self.embedding_dim, padding_idx=0)\n",
    "        # RNN层\n",
    "        self.bilstm_layer = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=self.bidirectional,\n",
    "                              batch_first=True)\n",
    "        # MLP注意力层\n",
    "        self.mlp_attention_layer = MLPAttentionNetwork(2 * self.hidden_dim, self.attention_dim)\n",
    "        # 全连接层\n",
    "        self.fc_layer = nn.Linear(2 * self.hidden_dim, self.num_classes)\n",
    "        # 单层softmax分类器\n",
    "        self.softmax_layer = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        :param x: 填充好的序列\n",
    "        :param lengths:\n",
    "        :return:\n",
    "        \"\"\"\n",
    " \n",
    "        # x: t.tensor([[1,2,3],[6,0,0],[4,5,0], [3, 7, 1]])\n",
    "        # lengths: t.tensor([3, 1, 2, 3])、序列的实际长度\n",
    "        max_len = lengths.max()\n",
    "        #x_input = self.embedding_layer(x)\n",
    "        #print(x)\n",
    "        x_packed_input = pack_padded_sequence(input=x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        # print(x_packed_input)\n",
    "        packed_out, _ = self.bilstm_layer(x_packed_input)\n",
    "        # print(packed_out)\n",
    "        outputs, _ = pad_packed_sequence(packed_out, batch_first=True, total_length= max_len, padding_value=0.0)\n",
    "        # print(out)\n",
    "        atten_scores, atten_out = self.mlp_attention_layer(outputs.permute(1, 0, 2), lengths)\n",
    "        atten_out = self.fc_layer(atten_out)\n",
    "        # (batch_size, num_classes)\n",
    "        # logits = self.softmax_layer(atten_out)\n",
    "        return atten_scores, atten_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252249d8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ec9f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:25.731954Z",
     "start_time": "2022-06-08T00:58:25.708707Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "num_node_type = 2\n",
    "num_edge_type = 2\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        companies_index,\n",
    "        transactions_index,\n",
    "        node_feature_size = 2,\n",
    "        transaction_feature_size = 8,\n",
    "        edge_type_number = 2,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True\n",
    "                ):\n",
    "        super(Net, self).__init__()\n",
    " \n",
    "        self.companies_index = companies_index\n",
    "        self.transactions_index = transactions_index\n",
    "        self.edge_type_number = edge_type_number\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(node_feature_size)\n",
    "        \n",
    "        conv = []\n",
    "        \n",
    "        self.edge_type_number = edge_type_number\n",
    "        self.node_feature_size = node_feature_size\n",
    "        self.transaction_feature_size = transaction_feature_size\n",
    "        for index in range(edge_type_number):\n",
    "            # Create the first GraphConv layer.\n",
    "            conv1 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            # Create the second GraphConv layer.\n",
    "            conv2 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            conv.append([conv1, conv2])\n",
    "        \n",
    "        self.conv = conv\n",
    "\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_feature * (2 + edge_type_number))\n",
    "        \n",
    "        self.postprocess_single = create_ffn(hidden_feature * (1 + edge_type_number))\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = torch.nn.Linear(hidden_feature, num_classes)\n",
    "        \n",
    "        # Create a process layer.\n",
    "        self.preprocess_2 = create_ffn(transaction_feature_size)\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1_2 = GraphConvLayer(\n",
    "            hidden_feature, hidden_feature\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.conv2_2 = GraphConvLayer(\n",
    "            hidden_feature, hidden_feature\n",
    "        )\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess_2 = create_ffn(hidden_feature * (3 + 2 * edge_type_number))\n",
    "        \n",
    "        self.postprocess_2_single = create_ffn(hidden_feature)\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits_2 = torch.nn.Linear(hidden_feature, num_classes)\n",
    "        \n",
    "        self.sequential = torch.nn.GRU(hidden_feature, hidden_feature, batch_first = True)\n",
    "        \n",
    "        self.b_a = BiLSTMAttentionNetwork(20, 16, 2, 1, bidirectional=True, attention_dim=5, num_classes=16)\n",
    "        \n",
    "    def forward(self, x, edge_index, node_mapping,transaction_node_mapping, companies_length, new_egde_type, x_2, edge_index2, explain_type='e', explain=False, single=False):\n",
    "        if explain_type=='t':\n",
    "            x, edge_index, x_2, edge_index2 = x_2, edge_index2, x, edge_index\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        x_m = x.clone()\n",
    "        \n",
    "        for index in range(self.edge_type_number):\n",
    "            edge = torch.stack([edge_index[0][new_egde_type == index],edge_index[1][new_egde_type == index]])\n",
    "            x_t = x.clone()\n",
    "            # Apply the first graph conv layer.\n",
    "            conv1  = self.conv[index][0]\n",
    "            x1 = conv1(x_t, edge)\n",
    "            # Skip connection.\n",
    "            x_t = x1 + x_t\n",
    "            # Apply the second graph conv layer.\n",
    "            conv2  = self.conv[index][1]\n",
    "            x2 = conv2(x_t, edge)\n",
    "            # Skip connection.\n",
    "            x_t = x2 + x_t\n",
    "            x_m = torch.cat([x_m, x_t], 1)\n",
    "        \n",
    "        x = x_m\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x_2 = self.preprocess_2(x_2)\n",
    "        # Apply the first graph conv layer.\n",
    "        x1_2 = self.conv1_2(x_2, edge_index2)\n",
    "        # Skip connection.\n",
    "        x_2 = x1_2 + x_2\n",
    "        # Apply the second graph conv layer.\n",
    "        x2_2 = self.conv2_2(x_2, edge_index2)\n",
    "        # Skip connection.\n",
    "        x_2 = x2_2 + x_2\n",
    "        \n",
    "        if not single:\n",
    "            if explain:\n",
    "                node_mapping_reverse = {node_mapping[index][0].item():index for index in range(len(node_mapping))}\n",
    "                transaction_node_mapping_reverse = {transaction_node_mapping[index][0].item():index for index in range(len(transaction_node_mapping))}\n",
    "\n",
    "                company_emb = []\n",
    "                for index in range(x_2.size(0)):\n",
    "                    if self.transactions_index[0][transaction_node_mapping[index][0].item()].item() in node_mapping_reverse:\n",
    "                        a = x[node_mapping_reverse[self.transactions_index[0][transaction_node_mapping[index][0].item()].item()]]\n",
    "                    else:\n",
    "                        a = torch.zeros((1 + self.edge_type_number)*hidden_feature).int()\n",
    "                    if self.transactions_index[1][transaction_node_mapping[index][0].item()].item() in node_mapping_reverse:\n",
    "                        b = x[node_mapping_reverse[self.transactions_index[1][transaction_node_mapping[index][0].item()].item()]]\n",
    "                    else:\n",
    "                        b = torch.zeros((1 + self.edge_type_number)*hidden_feature).int()\n",
    "                    company_emb.append(torch.cat((a,b),0))\n",
    "\n",
    "                company_emb = torch.stack(company_emb)\n",
    "            else:\n",
    "                company_emb = torch.cat([x[self.transactions_index[0]], x[self.transactions_index[1]]],1)\n",
    "\n",
    "            transaction_emb = []\n",
    "\n",
    "            \n",
    "            zero = torch.zeros((1, hidden_feature)).int()\n",
    "            new_tran = x_2.clone()\n",
    "            new_tran = torch.cat((zero, new_tran) ,dim=0)\n",
    "            \n",
    "            if explain:\n",
    "                for index in range(x.size(0)):\n",
    "                    if explain_type == 'e':\n",
    "                        comp = companies_index[node_mapping[index][0]]\n",
    "                    else:\n",
    "                        comp = companies_index[index]\n",
    "                    t_emb = []\n",
    "                    for c in comp:\n",
    "                        if c==0:\n",
    "                            t_emb.append(new_tran[0])\n",
    "                        else:\n",
    "                            c = c - 1\n",
    "                            if c in transaction_node_mapping_reverse:\n",
    "                                t_emb.append(x_2[transaction_node_mapping_reverse[c]])\n",
    "                            else:\n",
    "                                t_emb.append(new_tran[0])\n",
    "                    t_emb = torch.stack(t_emb)\n",
    "                    transaction_emb.append(t_emb)\n",
    "            else:\n",
    "                transaction_emb = new_tran[self.companies_index]\n",
    "                transaction_emb = list(transaction_emb)\n",
    "            \n",
    "            companies_length_clamped = companies_length.clamp(min=1, max=companies_length.max().item())\n",
    "            transaction_emb = torch.nn.utils.rnn.pad_sequence(transaction_emb, batch_first=True)\n",
    "            #transaction_emb = torch.nn.utils.rnn.pack_padded_sequence(transaction_emb, companies_length_clamped, batch_first=True, enforce_sorted=False)\n",
    "    #         _, transaction_emb = self.sequential(transaction_emb)\n",
    "\n",
    "            atten_scores, transaction_emb = self.b_a(transaction_emb, companies_length_clamped)\n",
    "        \n",
    "        \n",
    "        if not single:\n",
    "            x = torch.cat([x, transaction_emb], 1)\n",
    "            x = self.postprocess(x)\n",
    "        else:\n",
    "            x = self.postprocess_single(x)\n",
    "\n",
    "        # add company embedding\n",
    "        if not single:\n",
    "            x_2 = torch.cat([x_2, company_emb], 1)\n",
    "            x_2 = self.postprocess_2(x_2)\n",
    "        else:\n",
    "            x_2 = self.postprocess_2_single(x_2)\n",
    "        \n",
    "        # Compute logits\n",
    "        if explain:\n",
    "            if explain_type=='t':\n",
    "                return self.compute_logits(x_2)\n",
    "            else:\n",
    "                return self.compute_logits(x)\n",
    "        else:\n",
    "            if not single:\n",
    "                return self.compute_logits(x), self.compute_logits_2(x_2), atten_scores\n",
    "            else:\n",
    "                return self.compute_logits(x), self.compute_logits_2(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740e8c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:58:29.288274Z",
     "start_time": "2022-06-08T00:58:29.265675Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "num_node_type = 2\n",
    "num_edge_type = 2\n",
    "\n",
    "class SingleCompNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        companies_index,\n",
    "        transactions_index,\n",
    "        node_feature_size = 2,\n",
    "        transaction_feature_size = 8,\n",
    "        edge_type_number = 2,\n",
    "        tran_edge_type_number = 1,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True\n",
    "                ):\n",
    "        super(SingleCompNet, self).__init__()\n",
    " \n",
    "        self.companies_index = companies_index\n",
    "        self.transactions_index = transactions_index\n",
    "        self.edge_type_number = edge_type_number\n",
    "        self.tran_edge_type_number = tran_edge_type_number\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.preprocess = create_ffn(node_feature_size)\n",
    "        \n",
    "        conv = []\n",
    "        \n",
    "        self.node_feature_size = node_feature_size\n",
    "        self.transaction_feature_size = transaction_feature_size\n",
    "        for index in range(edge_type_number):\n",
    "            # Create the first GraphConv layer.\n",
    "            conv1 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            # Create the second GraphConv layer.\n",
    "            conv2 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            conv.append([conv1, conv2])\n",
    "        \n",
    "        self.conv = conv\n",
    "\n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_feature * (2 + edge_type_number))\n",
    "        \n",
    "        self.postprocess_single = create_ffn(hidden_feature * (1 + edge_type_number))\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits = torch.nn.Linear(hidden_feature, num_classes)\n",
    "        \n",
    "        # Create a process layer.\n",
    "        self.preprocess_2 = create_ffn(transaction_feature_size)\n",
    "\n",
    "        conv_2 = []\n",
    "        for index in range(tran_edge_type_number):\n",
    "            # Create the first GraphConv layer.\n",
    "            conv1 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            # Create the second GraphConv layer.\n",
    "            conv2 = GraphConvLayer(\n",
    "                hidden_feature, hidden_feature\n",
    "            )\n",
    "            conv_2.append([conv1, conv2])\n",
    "        \n",
    "        self.conv_2 = conv_2\n",
    "        \n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess_2 = create_ffn(hidden_feature * (3 + 3 * tran_edge_type_number))\n",
    "        \n",
    "        self.postprocess_2_single = create_ffn(hidden_feature*(1 + tran_edge_type_number))\n",
    "        # Create a compute logits layer.\n",
    "        self.compute_logits_2 = torch.nn.Linear(hidden_feature, num_classes)\n",
    "        \n",
    "        self.sequential = torch.nn.GRU(hidden_feature, hidden_feature, batch_first = True)\n",
    "        \n",
    "        self.b_a = BiLSTMAttentionNetwork(20, 32, 2, 1, bidirectional=True, attention_dim=5, num_classes=16)\n",
    "        \n",
    "    def forward(self, x, edge_index, node_mapping,transaction_node_mapping, companies_length, new_egde_type, x_2, edge_index2, tran_new_egde_type, explain_type='e', explain=False, single=False):\n",
    "        if explain_type=='t':\n",
    "            x, edge_index, x_2, edge_index2 = x_2, edge_index2, x, edge_index\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        x_m = x.clone()\n",
    "        \n",
    "        for index in range(self.edge_type_number):\n",
    "            edge = torch.stack([edge_index[0][new_egde_type == index],edge_index[1][new_egde_type == index]])\n",
    "            x_t = x.clone()\n",
    "            # Apply the first graph conv layer.\n",
    "            conv1  = self.conv[index][0]\n",
    "            x1 = conv1(x_t, edge)\n",
    "            # Skip connection.\n",
    "            x_t = x1 + x_t\n",
    "            # Apply the second graph conv layer.\n",
    "            conv2  = self.conv[index][1]\n",
    "            x2 = conv2(x_t, edge)\n",
    "            # Skip connection.\n",
    "            x_t = x2 + x_t\n",
    "            x_m = torch.cat([x_m, x_t], 1)\n",
    "        \n",
    "        x = x_m\n",
    "        \n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        x_2 = self.preprocess_2(x_2)\n",
    "        \n",
    "        x_m_2 = x_2.clone()\n",
    "        \n",
    "        for index in range(self.tran_edge_type_number):\n",
    "            edge = torch.stack([edge_index2[0][tran_new_egde_type == index],edge_index2[1][tran_new_egde_type == index]])\n",
    "            x_t = x_2.clone()\n",
    "            conv1_2  = self.conv_2[index][0]\n",
    "            x1_2 = conv1_2(x_t, edge)\n",
    "            x_t = x1_2 + x_t\n",
    "            conv2_2  = self.conv_2[index][1]\n",
    "            x2_2 = conv2_2(x_t, edge)\n",
    "            x_t = x2_2 + x_t\n",
    "            x_m_2 = torch.cat([x_m_2, x_t], 1)\n",
    "        \n",
    "        x_2 = x_m_2\n",
    "        \n",
    "\n",
    "        if not single:\n",
    "            if explain:\n",
    "                node_mapping_reverse = {node_mapping[index][0].item():index for index in range(len(node_mapping))}\n",
    "                transaction_node_mapping_reverse = {transaction_node_mapping[index][0].item():index for index in range(len(transaction_node_mapping))}\n",
    "\n",
    "                company_emb = []\n",
    "                for index in range(x_2.size(0)):\n",
    "                    if self.transactions_index[transaction_node_mapping[index][0].item()].item() in node_mapping_reverse:\n",
    "                        a = x[node_mapping_reverse[self.transactions_index[transaction_node_mapping[index][0].item()].item()]]\n",
    "                    else:\n",
    "                        a = torch.zeros((1 + self.edge_type_number)*hidden_feature).int()\n",
    "                    company_emb.append(a)\n",
    "\n",
    "                company_emb = torch.stack(company_emb)\n",
    "            else:\n",
    "                company_emb = x[self.transactions_index]\n",
    "\n",
    "            transaction_emb = []\n",
    "                        \n",
    "            zero = torch.zeros((1, 2*hidden_feature)).int()\n",
    "            new_tran = x_2.clone()\n",
    "            new_tran = torch.cat((zero, new_tran) ,dim=0)\n",
    "            \n",
    "            if explain:\n",
    "                for index in range(x.size(0)):\n",
    "                    if explain_type == 'e':\n",
    "                        comp = companies_index[node_mapping[index][0]]\n",
    "                    else:\n",
    "                        comp = companies_index[index]\n",
    "                    t_emb = []\n",
    "                    for c in comp:\n",
    "                        if c==0:\n",
    "                            t_emb.append(new_tran[0])\n",
    "                        else:\n",
    "                            c = c - 1\n",
    "                            if c in transaction_node_mapping_reverse:\n",
    "                                t_emb.append(x_2[transaction_node_mapping_reverse[c]])\n",
    "                            else:\n",
    "                                t_emb.append(new_tran[0])\n",
    "                    t_emb = torch.stack(t_emb)\n",
    "                    transaction_emb.append(t_emb)\n",
    "            else:\n",
    "                transaction_emb = new_tran[self.companies_index]\n",
    "                transaction_emb = list(transaction_emb)\n",
    "\n",
    "            companies_length_clamped = companies_length.clamp(min=1, max=companies_length.max().item())\n",
    "            transaction_emb = torch.nn.utils.rnn.pad_sequence(transaction_emb, batch_first=True)\n",
    "\n",
    "            atten_scores, transaction_emb = self.b_a(transaction_emb, companies_length_clamped)\n",
    "        \n",
    "        if not single:\n",
    "            x = torch.cat([x, transaction_emb], 1)\n",
    "            x = self.postprocess(x)\n",
    "        else:\n",
    "            x = self.postprocess_single(x)\n",
    "        \n",
    "        # add company embedding\n",
    "        if not single:\n",
    "            x_2 = torch.cat([x_2, company_emb], 1)\n",
    "            x_2 = self.postprocess_2(x_2)\n",
    "        else:\n",
    "            x_2 = self.postprocess_2_single(x_2)\n",
    "        \n",
    "        # Compute logits\n",
    "        if explain:\n",
    "            if explain_type=='t':\n",
    "                return self.compute_logits(x_2)\n",
    "            else:\n",
    "                return self.compute_logits(x)\n",
    "        else:\n",
    "            if not single:\n",
    "                return self.compute_logits(x), self.compute_logits_2(x_2), atten_scores\n",
    "            else:\n",
    "                return self.compute_logits(x), self.compute_logits_2(x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26827eb",
   "metadata": {},
   "source": [
    "## GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1733d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T02:24:27.475238Z",
     "start_time": "2022-06-06T02:24:27.438986Z"
    }
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "from math import sqrt\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "\n",
    "\n",
    "def get_num_hops(model: torch.nn.Module) -> int:\n",
    "    r\"\"\"Returns the number of hops the model is aggregating information\n",
    "    from.\"\"\"\n",
    "    from torch_geometric.nn.conv import MessagePassing\n",
    "    num_hops = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            num_hops += 1\n",
    "    return num_hops\n",
    "\n",
    "def set_masks(model: torch.nn.Module, mask: Tensor, edge_index: Tensor,\n",
    "              apply_sigmoid: bool = True):\n",
    "    \"\"\"Apply mask to every graph layer in the model.\"\"\"\n",
    "    loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "    # Loop over layers and set masks on MessagePassing layers:\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            module.explain = True\n",
    "            module._edge_mask = mask\n",
    "            module._loop_mask = loop_mask\n",
    "            module._apply_sigmoid = apply_sigmoid\n",
    "\n",
    "\n",
    "def clear_masks(model: torch.nn.Module):\n",
    "    \"\"\"Clear all masks from the model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            module.explain = False\n",
    "            module._edge_mask = None\n",
    "            module._loop_mask = None\n",
    "            module._apply_sigmoid = True\n",
    "    return module\n",
    "\n",
    "\n",
    "class CaptumModel(torch.nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module, mask_type: str = \"edge\",\n",
    "                 output_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        assert mask_type in ['edge', 'node', 'node_and_edge']\n",
    "\n",
    "        self.mask_type = mask_type\n",
    "        self.model = model\n",
    "        self.output_idx = output_idx\n",
    "\n",
    "    def forward(self, mask, *args):\n",
    "        \"\"\"\"\"\"\n",
    "        # The mask tensor, which comes from Captum's attribution methods,\n",
    "        # contains the number of samples in dimension 0. Since we are\n",
    "        # working with only one sample, we squeeze the tensors below.\n",
    "        assert mask.shape[0] == 1, \"Dimension 0 of input should be 1\"\n",
    "        if self.mask_type == \"edge\":\n",
    "            assert len(args) >= 2, \"Expects at least x and edge_index as args.\"\n",
    "        if self.mask_type == \"node\":\n",
    "            assert len(args) >= 1, \"Expects at least edge_index as args.\"\n",
    "        if self.mask_type == \"node_and_edge\":\n",
    "            assert args[0].shape[0] == 1, \"Dimension 0 of input should be 1\"\n",
    "            assert len(args[1:]) >= 1, \"Expects at least edge_index as args.\"\n",
    "\n",
    "        # Set edge mask:\n",
    "        if self.mask_type == 'edge':\n",
    "            set_masks(self.model, mask.squeeze(0), args[1],\n",
    "                      apply_sigmoid=False)\n",
    "        elif self.mask_type == 'node_and_edge':\n",
    "            set_masks(self.model, args[0].squeeze(0), args[1],\n",
    "                      apply_sigmoid=False)\n",
    "            args = args[1:]\n",
    "\n",
    "        if self.mask_type == 'edge':\n",
    "            x = self.model(*args)\n",
    "\n",
    "        elif self.mask_type == 'node':\n",
    "            x = self.model(mask.squeeze(0), *args)\n",
    "\n",
    "        else:\n",
    "            x = self.model(mask[0], *args)\n",
    "\n",
    "        # Clear mask:\n",
    "        if self.mask_type in ['edge', 'node_and_edge']:\n",
    "            clear_masks(self.model)\n",
    "\n",
    "        if self.output_idx is not None:\n",
    "            x = x[self.output_idx].unsqueeze(0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def to_captum(model: torch.nn.Module, mask_type: str = \"edge\",\n",
    "              output_idx: Optional[int] = None) -> torch.nn.Module:\n",
    "    r\"\"\"Converts a model to a model that can be used for\n",
    "    `Captum.ai <https://captum.ai/>`_ attribution methods.\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from captum.attr import IntegratedGradients\n",
    "        from torch_geometric.nn import GCN, from_captum\n",
    "\n",
    "        model = GCN(...)\n",
    "        ...  # Train the model.\n",
    "\n",
    "        # Explain predictions for node `10`:\n",
    "        output_idx = 10\n",
    "\n",
    "        captum_model = to_captum(model, mask_type=\"edge\",\n",
    "                                 output_idx=output_idx)\n",
    "        edge_mask = torch.ones(num_edges, requires_grad=True, device=device)\n",
    "\n",
    "        ig = IntegratedGradients(captum_model)\n",
    "        ig_attr = ig.attribute(edge_mask.unsqueeze(0),\n",
    "                               target=int(y[output_idx]),\n",
    "                               additional_forward_args=(x, edge_index),\n",
    "                               internal_batch_size=1)\n",
    "\n",
    "    .. note::\n",
    "        For an example of using a Captum attribution method within PyG, see\n",
    "        `examples/captum_explainability.py\n",
    "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        captum_explainability.py>`_.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be explained.\n",
    "        mask_type (str, optional): Denotes the type of mask to be created with\n",
    "            a Captum explainer. Valid inputs are :obj:`\"edge\"`, :obj:`\"node\"`,\n",
    "            and :obj:`\"node_and_edge\"`:\n",
    "\n",
    "            1. :obj:`\"edge\"`: The inputs to the forward function should be an\n",
    "               edge mask tensor of shape :obj:`[1, num_edges]`, a regular\n",
    "               :obj:`x` matrix and a regular :obj:`edge_index` matrix.\n",
    "\n",
    "            2. :obj:`\"node\"`: The inputs to the forward function should be a\n",
    "               node feature tensor of shape :obj:`[1, num_nodes, num_features]`\n",
    "               and a regular :obj:`edge_index` matrix.\n",
    "\n",
    "            3. :obj:`\"node_and_edge\"`: The inputs to the forward function\n",
    "               should be a node feature tensor of shape\n",
    "               :obj:`[1, num_nodes, num_features]`, an edge mask tensor of\n",
    "               shape :obj:`[1, num_edges]` and a regular :obj:`edge_index`\n",
    "               matrix.\n",
    "\n",
    "            For all mask types, additional arguments can be passed to the\n",
    "            forward function as long as the first arguments are set as\n",
    "            described. (default: :obj:`\"edge\"`)\n",
    "        output_idx (int, optional): Index of the output element (node or link\n",
    "            index) to be explained. With :obj:`output_idx` set, the forward\n",
    "            function will return the output of the model for the element at\n",
    "            the index specified. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    return CaptumModel(model, mask_type, output_idx)\n",
    "\n",
    "\n",
    "class ModifiedExplainer(torch.nn.Module):\n",
    "    r\"\"\"An abstract class for integrating explainability into Graph Neural\n",
    "    Networks, *e.g.* :class:`~torch_geometric.nn.GNNExplainer` and\n",
    "    :class:`~torch_geometric.nn.PGExplainer`.\n",
    "    It also provides general visualization methods for graph attributions.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`None`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`None`)\n",
    "        num_hops (int, optional): The number of hops the :obj:`model` is\n",
    "            aggregating information from.\n",
    "            If set to :obj:`None`, will automatically try to detect this\n",
    "            information based on the number of\n",
    "            :class:`~torch_geometric.nn.conv.message_passing.MessagePassing`\n",
    "            layers inside :obj:`model`. (default: :obj:`None`)\n",
    "        return_type (str, optional): Denotes the type of output from\n",
    "            :obj:`model`. Valid inputs are :obj:`\"log_prob\"` (the model\n",
    "            returns the logarithm of probabilities), :obj:`\"prob\"` (the\n",
    "            model returns probabilities), :obj:`\"raw\"` (the model returns raw\n",
    "            scores) and :obj:`\"regression\"` (the model returns scalars).\n",
    "            (default: :obj:`\"log_prob\"`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "            progress. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, lr: Optional[float] = None,\n",
    "                 epochs: Optional[int] = None, num_hops: Optional[int] = None,\n",
    "                 return_type: str = 'log_prob', log: bool = False):\n",
    "        super().__init__()\n",
    "        assert return_type in ['log_prob', 'prob', 'raw', 'regression']\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.num_hops = num_hops or get_num_hops(self.model)\n",
    "        self.return_type = return_type\n",
    "        self.log = log\n",
    "\n",
    "    def _flow(self) -> str:\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def subgraph(self, node_idx: int, x: Tensor, edge_index: Tensor, **kwargs):\n",
    "        r\"\"\"Returns the subgraph of the given node.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node to explain.\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (Tensor, Tensor, LongTensor, LongTensor, LongTensor, dict)\n",
    "        \"\"\"\n",
    "        num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "        subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=num_nodes, flow=self._flow())\n",
    "\n",
    "        x = x[subset]\n",
    "        kwargs_new = {}\n",
    "        for key, value in kwargs.items():\n",
    "            if torch.is_tensor(value) and value.size(0) == num_nodes:\n",
    "                kwargs_new[key] = value[subset]\n",
    "            elif torch.is_tensor(value) and value.size(0) == num_edges:\n",
    "                kwargs_new[key] = value[edge_mask]\n",
    "            else:\n",
    "                kwargs_new[key] = value  # TODO: this is not in PGExplainer\n",
    "        return x, edge_index, mapping, edge_mask, subset, kwargs_new\n",
    "\n",
    "    def _to_log_prob(self, x):\n",
    "        x = x.log_softmax(dim=-1) if self.return_type == 'raw' else x\n",
    "        x = x.log() if self.return_type == 'prob' else x\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_initial_prediction(self, x: Tensor, edge_index: Tensor,\n",
    "                               batch: Optional[Tensor] = None, **kwargs):\n",
    "        if batch is not None:\n",
    "            out = self.model(x, edge_index, batch, **kwargs)\n",
    "        else:\n",
    "            out = self.model(x, edge_index, **kwargs)\n",
    "        if self.return_type == 'regression':\n",
    "            prediction = out\n",
    "        else:\n",
    "            log_logits = self._to_log_prob(out)\n",
    "            prediction = log_logits.argmax(dim=-1)\n",
    "        return prediction\n",
    "\n",
    "    def get_loss(self, out: Tensor, prediction: Tensor,\n",
    "                 node_idx: Optional[int] = None, **kwargs):\n",
    "        if self.return_type == 'regression':\n",
    "            loss = self._loss(out, prediction, node_idx, **kwargs)\n",
    "        else:\n",
    "            log_logits = self._to_log_prob(out)\n",
    "            loss = self._loss(log_logits, prediction, node_idx, **kwargs)\n",
    "        return loss\n",
    "\n",
    "    def visualize_subgraph(self, node_idx: Optional[int], edge_index: Tensor,\n",
    "                           edge_mask: Tensor, y: Optional[Tensor] = None,\n",
    "                           threshold: Optional[int] = None,\n",
    "                           edge_y: Optional[Tensor] = None,\n",
    "                           node_alpha: Optional[Tensor] = None, seed: int = 10,\n",
    "                           **kwargs):\n",
    "        r\"\"\"Visualizes the subgraph given an edge mask :attr:`edge_mask`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node id to explain.\n",
    "                Set to :obj:`None` to explain a graph.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            edge_mask (Tensor): The edge mask.\n",
    "            y (Tensor, optional): The ground-truth node-prediction labels used\n",
    "                as node colorings. All nodes will have the same color\n",
    "                if :attr:`node_idx` is :obj:`-1`.(default: :obj:`None`).\n",
    "            threshold (float, optional): Sets a threshold for visualizing\n",
    "                important edges. If set to :obj:`None`, will visualize all\n",
    "                edges with transparancy indicating the importance of edges.\n",
    "                (default: :obj:`None`)\n",
    "            edge_y (Tensor, optional): The edge labels used as edge colorings.\n",
    "            node_alpha (Tensor, optional): Tensor of floats (0 - 1) indicating\n",
    "                transparency of each node.\n",
    "            seed (int, optional): Random seed of the :obj:`networkx` node\n",
    "                placement algorithm. (default: :obj:`10`)\n",
    "            **kwargs (optional): Additional arguments passed to\n",
    "                :func:`nx.draw`.\n",
    "\n",
    "        :rtype: :class:`matplotlib.axes.Axes`, :class:`networkx.DiGraph`\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import networkx as nx\n",
    "\n",
    "        assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "#         if node_idx is None or node_idx < 0:\n",
    "#             hard_edge_mask = torch.BoolTensor([True] * edge_index.size(1),\n",
    "#                                               device=edge_mask.device)\n",
    "#             subset = torch.arange(edge_index.max().item() + 1,\n",
    "#                                   device=edge_index.device)\n",
    "#             y = None\n",
    "\n",
    "#         else:\n",
    "            # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        subset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=None, flow=self._flow())\n",
    "\n",
    "        edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "        if threshold is not None:\n",
    "            edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "\n",
    "        if y is None:\n",
    "            y = torch.zeros(edge_index.max().item() + 1,\n",
    "                            device=edge_index.device)\n",
    "        else:\n",
    "            y = y[subset].to(torch.float) / y.max().item()\n",
    "\n",
    "        if edge_y is None:\n",
    "            edge_color = ['black'] * edge_index.size(1)\n",
    "        else:\n",
    "            colors = list(plt.rcParams['axes.prop_cycle'])\n",
    "            edge_color = [\n",
    "                colors[i % len(colors)]['color']\n",
    "                for i in edge_y[hard_edge_mask]\n",
    "            ]\n",
    "\n",
    "        data = Data(edge_index=edge_index, att=edge_mask,\n",
    "                    edge_color=edge_color, y=y, num_nodes=y.size(0)).to('cpu')\n",
    "        G = to_networkx(data, node_attrs=['y'],\n",
    "                        edge_attrs=['att', 'edge_color'])\n",
    "        mapping = {k: i for k, i in enumerate(subset.tolist())}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        node_args = set(signature(nx.draw_networkx_nodes).parameters.keys())\n",
    "        node_kwargs = {k: v for k, v in kwargs.items() if k in node_args}\n",
    "        node_kwargs['node_size'] = kwargs.get('node_size') or 800\n",
    "        node_kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "        label_args = set(signature(nx.draw_networkx_labels).parameters.keys())\n",
    "        label_kwargs = {k: v for k, v in kwargs.items() if k in label_args}\n",
    "        label_kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "\n",
    "        pos = nx.spring_layout(G, seed=seed)\n",
    "        ax = plt.gca()\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            ax.annotate(\n",
    "                '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "                textcoords='data', arrowprops=dict(\n",
    "                    arrowstyle=\"->\",\n",
    "                    alpha=max(data['att'], 0.1),\n",
    "                    color=data['edge_color'],\n",
    "                    shrinkA=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    shrinkB=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    connectionstyle=\"arc3,rad=0.1\",\n",
    "                ))\n",
    "\n",
    "        if node_alpha is None:\n",
    "            nx.draw_networkx_nodes(G, pos, node_color=y.tolist(),\n",
    "                                   **node_kwargs)\n",
    "        else:\n",
    "            node_alpha_subset = node_alpha[subset]\n",
    "            assert ((node_alpha_subset >= 0) & (node_alpha_subset <= 1)).all()\n",
    "            nx.draw_networkx_nodes(G, pos, alpha=node_alpha_subset.tolist(),\n",
    "                                   node_color=y.tolist(), **node_kwargs)\n",
    "\n",
    "        nx.draw_networkx_labels(G, pos, **label_kwargs)\n",
    "\n",
    "        return ax, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424d60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T02:24:30.323553Z",
     "start_time": "2022-06-06T02:24:30.295366Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "def set_masks(model: torch.nn.Module, mask: Tensor, edge_index: Tensor,\n",
    "              apply_sigmoid: bool = True):\n",
    "    \"\"\"Apply mask to every graph layer in the model.\"\"\"\n",
    "    loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "    # Loop over layers and set masks on MessagePassing layers:\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            module.explain = True\n",
    "            module._edge_mask = mask\n",
    "            module._loop_mask = loop_mask\n",
    "            module._apply_sigmoid = apply_sigmoid\n",
    "\n",
    "\n",
    "def clear_masks(model: torch.nn.Module):\n",
    "    \"\"\"Clear all masks from the model.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            module.explain = False\n",
    "            module._edge_mask = None\n",
    "            module._loop_mask = None\n",
    "            module._apply_sigmoid = True\n",
    "    return module\n",
    "\n",
    "class ModifiedGNNExplainer(ModifiedExplainer):\n",
    "    r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "    Explanations for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "    structures and small subsets node features that play a crucial role in a\n",
    "    GNN’s node-predictions.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using GNN-Explainer, see `examples/gnn_explainer.py\n",
    "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        gnn_explainer.py>`_.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        num_hops (int, optional): The number of hops the :obj:`model` is\n",
    "            aggregating information from.\n",
    "            If set to :obj:`None`, will automatically try to detect this\n",
    "            information based on the number of\n",
    "            :class:`~torch_geometric.nn.conv.message_passing.MessagePassing`\n",
    "            layers inside :obj:`model`. (default: :obj:`None`)\n",
    "        return_type (str, optional): Denotes the type of output from\n",
    "            :obj:`model`. Valid inputs are :obj:`\"log_prob\"` (the model\n",
    "            returns the logarithm of probabilities), :obj:`\"prob\"` (the\n",
    "            model returns probabilities), :obj:`\"raw\"` (the model returns raw\n",
    "            scores) and :obj:`\"regression\"` (the model returns scalars).\n",
    "            (default: :obj:`\"log_prob\"`)\n",
    "        feat_mask_type (str, optional): Denotes the type of feature mask\n",
    "            that will be learned. Valid inputs are :obj:`\"feature\"` (a single\n",
    "            feature-level mask for all nodes), :obj:`\"individual_feature\"`\n",
    "            (individual feature-level masks for each node), and :obj:`\"scalar\"`\n",
    "            (scalar mask for each each node). (default: :obj:`\"feature\"`)\n",
    "        allow_edge_mask (boolean, optional): If set to :obj:`False`, the edge\n",
    "            mask will not be optimized. (default: :obj:`True`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "            progress. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional hyper-parameters to override default\n",
    "            settings in :attr:`~torch_geometric.nn.models.GNNExplainer.coeffs`.\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, epochs: int = 100, lr: float = 0.01,\n",
    "                 num_hops: Optional[int] = None, return_type: str = 'log_prob',\n",
    "                 feat_mask_type: str = 'feature', allow_edge_mask: bool = True,\n",
    "                 log: bool = True, **kwargs):\n",
    "        super().__init__(model, lr, epochs, num_hops, return_type, log)\n",
    "        assert feat_mask_type in ['feature', 'individual_feature', 'scalar']\n",
    "        self.allow_edge_mask = allow_edge_mask\n",
    "        self.feat_mask_type = feat_mask_type\n",
    "        self.coeffs.update(kwargs)\n",
    "\n",
    "    def _initialize_masks(self, x, edge_index, init=\"normal\"):\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "        std = 0.1\n",
    "\n",
    "        if self.feat_mask_type == 'individual_feature':\n",
    "            self.node_feat_mask = torch.nn.Parameter(torch.randn(N, F) * std)\n",
    "        elif self.feat_mask_type == 'scalar':\n",
    "            self.node_feat_mask = torch.nn.Parameter(torch.randn(N, 1) * std)\n",
    "        else:\n",
    "            self.node_feat_mask = torch.nn.Parameter(torch.randn(1, F) * std)\n",
    "\n",
    "        std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "\n",
    "        if self.allow_edge_mask:\n",
    "            self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "\n",
    "    def _clear_masks(self):\n",
    "        clear_masks(self.model)\n",
    "        self.node_feat_masks = None\n",
    "        self.edge_mask = None\n",
    "\n",
    "    def _loss(self, log_logits, prediction, node_idx: Optional[int] = None):\n",
    "        if self.return_type == 'regression':\n",
    "            if node_idx is not None and node_idx >= 0:\n",
    "                loss = torch.cdist(log_logits[node_idx], prediction[node_idx])\n",
    "            else:\n",
    "                loss = torch.cdist(log_logits, prediction)\n",
    "        else:\n",
    "            if node_idx is not None and node_idx >= 0:\n",
    "                loss = -log_logits[node_idx, prediction[node_idx]]\n",
    "            else:\n",
    "                loss = -log_logits[0, prediction[0]]\n",
    "\n",
    "        if self.allow_edge_mask:\n",
    "            m = self.edge_mask.sigmoid()\n",
    "            edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "            loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "            ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "            loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        m = self.node_feat_mask.sigmoid()\n",
    "        node_feat_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        loss = loss + self.coeffs['node_feat_size'] * node_feat_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def explain_graph(self, x, edge_index, **kwargs):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for a graph.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self._clear_masks()\n",
    "\n",
    "        # all nodes belong to same graph\n",
    "        batch = torch.zeros(x.shape[0], dtype=int, device=x.device)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        prediction = self.get_initial_prediction(x, edge_index, batch=batch,\n",
    "                                                 **kwargs)\n",
    "\n",
    "        self._initialize_masks(x, edge_index)\n",
    "        self.to(x.device)\n",
    "        if self.allow_edge_mask:\n",
    "            set_masks(self.model, self.edge_mask, edge_index,\n",
    "                      apply_sigmoid=True)\n",
    "            parameters = [self.node_feat_mask, self.edge_mask]\n",
    "        else:\n",
    "            parameters = [self.node_feat_mask]\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.epochs)\n",
    "            pbar.set_description('Explain graph')\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.node_feat_mask.sigmoid()\n",
    "            out1,out2,att_score = self.model(x=h, edge_index=edge_index, batch=batch, **kwargs)\n",
    "            loss = self.get_loss(out1, prediction, None)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.log:  # pragma: no cover\n",
    "                pbar.update(1)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.detach().sigmoid().squeeze()\n",
    "        if self.allow_edge_mask:\n",
    "            edge_mask = self.edge_mask.detach().sigmoid()\n",
    "        else:\n",
    "            edge_mask = torch.ones(edge_index.size(1))\n",
    "\n",
    "        self._clear_masks()\n",
    "        return node_feat_mask, edge_mask\n",
    "\n",
    "    def explain_node(self, node_idx, x, edge_index, **kwargs):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for node\n",
    "        :attr:`node_idx`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node to explain.\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self._clear_masks()\n",
    "\n",
    "        num_nodes = x.size(0)\n",
    "        num_edges = edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        x, edge_index, mapping, hard_edge_mask, subset, kwargs = \\\n",
    "            self.subgraph(node_idx, x, edge_index, **kwargs)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        prediction = self.get_initial_prediction(x, edge_index, **kwargs)\n",
    "\n",
    "        self._initialize_masks(x, edge_index)\n",
    "        self.to(x.device)\n",
    "\n",
    "        if self.allow_edge_mask:\n",
    "            set_masks(self.model, self.edge_mask, edge_index,\n",
    "                      apply_sigmoid=True)\n",
    "            parameters = [self.node_feat_mask, self.edge_mask]\n",
    "        else:\n",
    "            parameters = [self.node_feat_mask]\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.epochs)\n",
    "            pbar.set_description(f'Explain node {node_idx}')\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.node_feat_mask.sigmoid()\n",
    "            out = self.model(x=h, edge_index=edge_index, **kwargs)\n",
    "            loss = self.get_loss(out, prediction, mapping)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.log:  # pragma: no cover\n",
    "                pbar.update(1)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.detach().sigmoid()\n",
    "        if self.feat_mask_type == 'individual_feature':\n",
    "            new_mask = x.new_zeros(num_nodes, x.size(-1))\n",
    "            new_mask[subset] = node_feat_mask\n",
    "            node_feat_mask = new_mask\n",
    "        elif self.feat_mask_type == 'scalar':\n",
    "            new_mask = x.new_zeros(num_nodes, 1)\n",
    "            new_mask[subset] = node_feat_mask\n",
    "            node_feat_mask = new_mask\n",
    "        node_feat_mask = node_feat_mask.squeeze()\n",
    "\n",
    "        if self.allow_edge_mask:\n",
    "            edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "            edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "        else:\n",
    "            edge_mask = torch.zeros(num_edges)\n",
    "            edge_mask[hard_edge_mask] = 1\n",
    "\n",
    "        self._clear_masks()\n",
    "\n",
    "        return node_feat_mask, edge_mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4694a2",
   "metadata": {},
   "source": [
    "## MultiFraud: Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e8f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T00:52:32.839292Z",
     "start_time": "2022-06-08T00:52:32.794975Z"
    }
   },
   "outputs": [],
   "source": [
    "saveData ={\n",
    "    \"companies_index\":companies_index,\n",
    "    \"transactions_index\":transactions_index,\n",
    "    \"company_node_feature\":company_node_feature,\n",
    "    \"new_edge_index\":new_edge_index,\n",
    "    \"node_mapping\":node_mapping,\n",
    "    \"transaction_node_mapping\":transaction_node_mapping,\n",
    "    \"companies_length\":companies_length,\n",
    "    \"new_egde_type\":new_egde_type,\n",
    "    \"transaction_node_feature\":transaction_node_feature,\n",
    "    \"new_edge_index_2\":new_edge_index_2,\n",
    "    \"real_trans_edge_type\":real_trans_edge_type,\n",
    "    \"x_train_new1\":x_train_new1,\n",
    "    \"y_train_new1\":y_train_new1,\n",
    "    \"x_train_new2\":x_train_new2,\n",
    "    \"y_train_new2\":y_train_new2,\n",
    "    \"x_test_new1\":x_test_new1,\n",
    "    \"y_test_new1\":y_test_new1,\n",
    "    \"x_test_new2\":x_test_new2,\n",
    "    \"y_test_new2\":y_test_new2,\n",
    "}\n",
    "sio.savemat(\"dataset/real_data_for_explain.mat\", saveData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5cade",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-07T14:44:03.093Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dual test\n",
    "explain_model = SingleCompNet(companies_index, transactions_index,  node_feature_size = 89, transaction_feature_size = 23, edge_type_number = 3, tran_edge_type_number = 1, dropout_rate=0.2).to(device)\n",
    "#explain_model = Net(companies_index, transactions_index,  node_feature_size = 8, transaction_feature_size = 8, edge_type_number = 1, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(explain_model.parameters(), lr=0.01)\n",
    "cost1D = []\n",
    "cost2D = []\n",
    "costD = []\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# checkpoint = torch.load(\"./checkpoint/checkpoint_209_epoch.pkl\")\n",
    "# explain_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# start_epoch = checkpoint['epoch']\n",
    "\n",
    "explain_model.train()\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out2, atten_scores = explain_model(company_node_feature, new_edge_index, node_mapping, transaction_node_mapping, companies_length, new_egde_type, transaction_node_feature, new_edge_index_2, real_trans_edge_type)\n",
    "    loss1 = loss_func(out1[x_train_new1] ,y_train_new1) \n",
    "    loss2 = loss_func(out2[x_train_new2] ,y_train_new2) \n",
    "    loss = (loss1+loss2)/2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cost1D.append(loss1)\n",
    "    cost2D.append(loss2)\n",
    "    costD.append(loss)\n",
    "    print('Iter-{}; Total loss: {:.4}'.format(epoch, loss))\n",
    "    \n",
    "#     if (epoch+1) % checkpoint_interval == 0:\n",
    "\n",
    "#         checkpoint = {\"model_state_dict\": explain_model.state_dict(),\n",
    "#                       \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                       \"epoch\": epoch,\n",
    "#                       \"loss1\": loss1,\"loss2\":loss2}\n",
    "#         path_checkpoint = \"./checkpoint/checkpoint_{}_epoch.pkl\".format(epoch)\n",
    "#         torch.save(checkpoint, path_checkpoint)\n",
    "\n",
    "# torch.save(explain_model.state_dict(), \"./checkpoint/checkpoint.pkl\")\n",
    "\n",
    "# print(atten_scores)\n",
    "\n",
    "plt.plot(np.squeeze(costD), '-b')\n",
    "plt.ylabel('total cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(np.squeeze(cost1D), '-y')\n",
    "plt.ylabel('task 1 cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(np.squeeze(cost2D),'-r')\n",
    "plt.ylabel('task 2 cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()\n",
    "\n",
    "explain_model.eval()\n",
    "\n",
    "out1, out2, test_atten_scores  = explain_model(company_node_feature, new_edge_index, node_mapping, transaction_node_mapping, companies_length, new_egde_type, transaction_node_feature, new_edge_index_2, real_trans_edge_type)\n",
    "pred1 = out1[x_test_new1].argmax(dim=1)\n",
    "pred2 = out2[x_test_new2].argmax(dim=1)\n",
    "print(classification_report(y_test_new1, pred1,digits=4))\n",
    "print(confusion_matrix(y_test_new1, pred1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_new1, pred1, pos_label=1)\n",
    "print(auc(fpr, tpr))\n",
    "print(classification_report(y_test_new2, pred2,digits=4))\n",
    "print(confusion_matrix(y_test_new2, pred2))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_new2, pred2, pos_label=1)\n",
    "print(auc(fpr, tpr))\n",
    "print(test_atten_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59742b",
   "metadata": {},
   "source": [
    "## MultiFraud-S: Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0aadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T08:29:50.006846Z",
     "start_time": "2022-05-31T08:29:43.339107Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single Test\n",
    "# single_explain_model = SingleCompNet(companies_index, transactions_index,  node_feature_size = 89, transaction_feature_size = 23, edge_type_number = 3, tran_edge_type_number = 2, dropout_rate=0.2).to(device)\n",
    "single_explain_model = Net(companies_index, transactions_index,  node_feature_size = 8, transaction_feature_size = 8, edge_type_number = 1, dropout_rate=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(single_explain_model.parameters(), lr=0.05)\n",
    "cost1D = []\n",
    "cost2D = []\n",
    "costD = []\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "single_explain_model.train()\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out2 = single_explain_model(company_node_feature, new_edge_index, node_mapping, transaction_node_mapping, companies_length, new_egde_type, transaction_node_feature, new_edge_index_2, real_trans_edge_type, single = True)\n",
    "    loss1 = loss_func(out1[x_train_new1] ,y_train_new1) \n",
    "    loss2 = loss_func(out2[x_train_new2] ,y_train_new2) \n",
    "    loss = (loss1+loss2)/2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cost1D.append(loss1)\n",
    "    cost2D.append(loss2)\n",
    "    costD.append(loss)\n",
    "    print('Iter-{}; Total loss: {:.4}'.format(epoch, loss))\n",
    "\n",
    "#print(atten_scores)\n",
    "\n",
    "plt.plot(np.squeeze(costD), '-b')\n",
    "plt.ylabel('total cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(np.squeeze(cost1D), '-y')\n",
    "plt.ylabel('task 1 cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show() \n",
    "\n",
    "plt.plot(np.squeeze(cost2D),'-r')\n",
    "plt.ylabel('task 2 cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()\n",
    "\n",
    "single_explain_model.eval()\n",
    "out1, out2  = single_explain_model(company_node_feature, new_edge_index, node_mapping, transaction_node_mapping, companies_length, new_egde_type, transaction_node_feature, new_edge_index_2,real_trans_edge_type,single = True)\n",
    "pred1 = out1[x_test_new1].argmax(dim=1)\n",
    "pred2 = out2[x_test_new2].argmax(dim=1)\n",
    "print(classification_report(y_test_new1, pred1, digits=4))\n",
    "print(confusion_matrix(y_test_new1, pred1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_new1, pred1, pos_label=1)\n",
    "print(auc(fpr, tpr))\n",
    "print(classification_report(y_test_new2, pred2, digits=4))\n",
    "print(confusion_matrix(y_test_new2, pred2))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_new2, pred2, pos_label=1)\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8118f",
   "metadata": {},
   "source": [
    "## Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3b283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T14:39:02.649046Z",
     "start_time": "2022-06-06T14:35:40.823137Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explainer = ModifiedGNNExplainer(explain_model, epochs=1000, num_hops=3)\n",
    "node_idx = 1023\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx,x = company_node_feature, edge_index = new_edge_index, node_mapping = node_mapping, transaction_node_mapping = transaction_node_mapping, companies_length=companies_length, new_egde_type = new_egde_type, x_2 = transaction_node_feature, edge_index2 = new_edge_index_2, explain = True)\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx,x = company_node_feature, edge_index = new_edge_index, node_mapping = node_mapping, transaction_node_mapping = transaction_node_mapping, companies_length=companies_length, new_egde_type = new_egde_type, x_2 = transaction_node_feature, edge_index2 = new_edge_index_2, tran_new_egde_type = real_trans_edge_type, explain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364f457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T14:39:04.208090Z",
     "start_time": "2022-06-06T14:39:03.529525Z"
    }
   },
   "outputs": [],
   "source": [
    "ax, explain_G = explainer.visualize_subgraph(node_idx, new_edge_index, edge_mask, y = torch.Tensor(role_id).int())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43532e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T02:00:38.299630Z",
     "start_time": "2022-06-07T02:00:38.272300Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057d915",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T02:00:39.146764Z",
     "start_time": "2022-06-07T02:00:39.112889Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_edge_source = new_edge_index[0][edge_mask>0]\n",
    "comp_edge_target = new_edge_index[1][edge_mask>0]\n",
    "comp_edge_type = new_egde_type[edge_mask>0]\n",
    "comp_edge_weights = edge_mask[edge_mask>0]\n",
    "\n",
    "comps = set()\n",
    "\n",
    "normal_comp_nodes = []\n",
    "fraud_comp_nodes = []\n",
    "normal_tran_nodes = []\n",
    "fraud_tran_nodes = []\n",
    "edge_types = []\n",
    "edge_weights = []\n",
    "\n",
    "\n",
    "comp_colors = [\"grey\", \"skyblue\", \"firebrick\"]\n",
    "for index, comp in enumerate(comp_edge_source):\n",
    "    #if comp.item() == node_idx or comp_edge_target[index] == node_idx:\n",
    "    comps.add(comp.item())\n",
    "    comps.add(comp_edge_target[index].item())\n",
    "    G.add_node('e' + str(comp.item()))\n",
    "    G.add_node('e' + str(comp_edge_target[index].item()))\n",
    "    if G.has_edge('e' + str(comp.item()), 'e' + str(comp_edge_target[index].item())):\n",
    "        G.add_edge('e' + str(comp.item()), 'e' + str(comp_edge_target[index].item()),color=comp_colors[1], weights = comp_edge_weights[index].item() + G['e' + str(comp.item())]['e' + str(comp_edge_target[index].item())]['weights'] )\n",
    "    else:\n",
    "        G.add_edge('e' + str(comp.item()), 'e' + str(comp_edge_target[index].item()),color=comp_colors[1], weights = comp_edge_weights[index].item())\n",
    "print(comps)\n",
    "for comp in comps:\n",
    "    if comp != node_idx:\n",
    "        if role_id[comp] == 1:\n",
    "            fraud_comp_nodes.append('e' + str(comp))\n",
    "        else:\n",
    "            normal_comp_nodes.append('e' + str(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44096c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T01:15:49.233047Z",
     "start_time": "2022-06-07T01:14:08.065862Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer = ModifiedGNNExplainer(explain_model, epochs=500, num_hops=3)\n",
    "t_node_idx = companies_index[node_idx][test_atten_scores[node_idx].argmax()] - 1 \n",
    "# node_feat_mask_2, edge_mask_2 = explainer.explain_node(t_node_idx ,x = transaction_node_feature, edge_index = new_edge_index_2, node_mapping = node_mapping, transaction_node_mapping = transaction_node_mapping, companies_length=companies_length, new_egde_type = new_egde_type, x_2 = company_node_feature, edge_index2 = new_edge_index,tran_new_egde_type = real_trans_edge_type, explain_type='t', explain = True)\n",
    "node_feat_mask_2, edge_mask_2 = explainer.explain_node(t_node_idx ,x = transaction_node_feature, edge_index = new_edge_index_2, node_mapping = node_mapping, transaction_node_mapping = transaction_node_mapping, companies_length=companies_length, new_egde_type = new_egde_type, x_2 = company_node_feature, edge_index2 = new_edge_index, explain_type='t', explain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e039cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T01:46:15.661988Z",
     "start_time": "2022-06-07T01:46:15.025910Z"
    }
   },
   "outputs": [],
   "source": [
    "ax, explain_G = explainer.visualize_subgraph(t_node_idx, new_edge_index_2, edge_mask_2, y = torch.Tensor(trans_role_id).int())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15206866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T02:00:44.207309Z",
     "start_time": "2022-06-07T02:00:44.170344Z"
    }
   },
   "outputs": [],
   "source": [
    "tran_edge_source = new_edge_index_2[0][edge_mask_2 > 0]\n",
    "tran_edge_target = new_edge_index_2[1][edge_mask_2 > 0]\n",
    "#tran_edge_type = real_trans_edge_type[edge_mask_2 > 0]\n",
    "tran_edge_weights = edge_mask_2[edge_mask_2 > 0]\n",
    "\n",
    "tran_edge_source = torch.cat([tran_edge_source, new_edge_index_2[0][edge_mask_3 > 0]])\n",
    "tran_edge_target = torch.cat([tran_edge_target,new_edge_index_2[1][edge_mask_3 > 0]])\n",
    "#tran_edge_type = real_trans_edge_type[edge_mask_2 > 0]\n",
    "tran_edge_weights = torch.cat([tran_edge_weights, edge_mask_3[edge_mask_3 > 0]])\n",
    "\n",
    "\n",
    "trans = set()\n",
    "tran_colors = [\"orange\", \"peru\"]\n",
    "for index, tran in enumerate(tran_edge_source):\n",
    "    #if tran.item() == t_node_idx or tran_edge_target[index] == t_node_idx:\n",
    "    trans.add(tran.item())\n",
    "    trans.add(tran_edge_target[index].item())\n",
    "    G.add_node('t' + str(tran.item()))\n",
    "    G.add_node('t' + str(tran_edge_target[index].item()))\n",
    "    if G.has_edge('t' + str(tran.item()), 't' + str(tran_edge_target[index].item())):\n",
    "        G.add_edge('t' + str(tran.item()), 't' + str(tran_edge_target[index].item()),color=tran_colors[0], weights = tran_edge_weights[index].item() + G['t' + str(tran.item())]['t' + str(tran_edge_target[index].item())]['weights'])\n",
    "    else:\n",
    "        G.add_edge('t' + str(tran.item()), 't' + str(tran_edge_target[index].item()),color=tran_colors[0], weights = tran_edge_weights[index].item())\n",
    "print(trans)\n",
    "for t in trans:\n",
    "#     if t!=t_node_idx:\n",
    "    if trans_role_id[t] == 1:\n",
    "        fraud_tran_nodes.append('t'+str(t))\n",
    "    else:\n",
    "        normal_tran_nodes.append('t'+str(t))\n",
    "print(fraud_tran_nodes)\n",
    "        \n",
    "G.add_edge('e' + str(node_idx), 't' + str(t_node_idx), color=\"green\", weights=1)\n",
    "G.add_edge('e' + str(node_idx), 't' + str(t_node_idx_2), color=\"green\", weights=0.9)\n",
    "\n",
    "edges = G.edges()\n",
    "colors = [G[u][v]['color'] for u,v in edges]\n",
    "weights = [G[v][u]['weights'] for u,v in edges]\n",
    "\n",
    "labels = {} \n",
    "for node in G.nodes():\n",
    "    labels[node] = node\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f60a40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T02:01:33.638211Z",
     "start_time": "2022-06-07T02:01:32.771735Z"
    }
   },
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=normal_comp_nodes, node_color='lightgrey', \n",
    "                           node_shape='s', label = 'normal_companies', node_size=1200, alpha = 0.8)\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=fraud_comp_nodes, node_color='red', \n",
    "                           node_shape='s', label = 'fraud_companies',node_size=1200,  alpha = 0.8)\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=normal_tran_nodes,node_size=800, node_color='lightgrey', \n",
    "                           node_shape='o', label = 'normal_transactions', alpha = 0.8)\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=fraud_tran_nodes,node_size=800, node_color='red', \n",
    "                           node_shape='o', label = 'fraud_transactions', alpha = 0.8)\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist= [ 'e'+ str(node_idx)],node_size=1200, node_color='red', node_shape='x', \n",
    "                           label = 'node_to_explain', linewidths = 8,  alpha = 0.8)\n",
    "\n",
    "ax = plt.gca()\n",
    "for source, target, data in G.edges(data=True):\n",
    "    ax.annotate(\n",
    "        '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "        textcoords='data', arrowprops=dict(\n",
    "            arrowstyle=\"-\",\n",
    "            linewidth=max(2 + 4*data['weights'], 2),\n",
    "            color=data['color'],\n",
    "            shrinkA=15,\n",
    "            shrinkB=15,\n",
    "            connectionstyle=\"arc3,rad=0.1\",\n",
    "        ))\n",
    "\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=28,font_color='black')\n",
    "\n",
    "plt.legend(scatterpoints = 1, loc = 'best', fontsize = 'xx-small', prop={'size':30})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgfraud",
   "language": "python",
   "name": "dgfraud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "293px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
